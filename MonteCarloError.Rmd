---
title: "Propagating Monte Carlo Error"
author: "Team A7"
date: "11/29/2018"
output:
  pdf_document: default
  html_document: default
fig_width: 2
fig_height: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(pander)
# Set up my table with provided data
mytab <- read_tsv("classA7.dat", col_names = FALSE)
mytab <- t(apply(mytab, c(1, 2), function(x) eval(parse(text = x))))
colnames(mytab) <- c("X", "Y")
rownames(mytab) <- NULL
```

The functions we are using to generate the fake data are:

1. $f_1(x) = -2 + 3x$
2. $f_2(x) = 3$
3. $f_3(x) = 6x^2 + 3x + 3$
4. $f_4(x) = 10x + 3$
5. $f_5(x) = -4x - 6$

```{r data-gen, echo=FALSE}
f1 <- function(x) -2 + (3*x)
f2 <- function(x) 3
f3 <- function(x) (6*(x**2)) + (3*x) + 3
f4 <- function(x) (10*x) + 3
f5 <- function(x) (-4 * x) - 6

makeFakeData <- function(f) {
    f(mytab[,1]) + rnorm(n = length(mytab[,1]), mean = 0, sd = 1)
}
```

Generation and plotting of the 1000 c0 and c1 values from fake data

```{r linear-fit, echo=FALSE, dev="png", dpi=300, results="asis", tidy=TRUE, out.width='50%'}
funclist <- list(f1, f2, f3, f4, f5)
means_slopes <- integer(length(funclist))
means_intercepts <- integer(length(funclist))
sd_slopes <- integer(length(funclist))
sd_intercepts <- integer(length(funclist))

#print.noquote(funclist[1])
func_ind <- 1
for (fn in funclist) {
  dat = matrix(nrow = 1000, ncol = 2)
  for ( k in 1:1000 ) {
    fakedat <- makeFakeData(fn)
    dat[k,] <- coef(lm( fakedat ~ mytab[,1] ))
  }

  colnames(dat) <- c("Intercept", "Slope")
  means_intercepts[func_ind] <- mean(dat[,"Intercept"])
  means_slopes[func_ind] <- mean(dat[,"Slope"])
  sd_intercepts[func_ind] <- sd(dat[,"Intercept"])
  sd_slopes[func_ind] <- sd(dat[,"Slope"])
  if (func_ind < 3) {
    pandoc.header(paste("Function number", func_ind), level = 2)
    plot(dat)
    densities <- density(dat[,"Intercept"], adjust = 2)
    h <- hist(dat[,"Intercept"], freq = FALSE,  main = "Density plot for y-intercept", xlim = c(min(fakedat), max(fakedat)))
    lines(densities, col = "red", lty=3)
    abline(a=0, b = 0)
    densities <- density(dat[,"Slope"], adjust = 2)
    hist(dat[,"Slope"], freq = FALSE,  main = "Density plot for slope")
    lines(densities, col = "red", lty=3)
    abline(a=0, b = 0)
    pandoc.p(paste("Mean of the intercept:", means_intercepts[func_ind]))
    pandoc.p(paste("Variance of the intercept:", sd_intercepts[func_ind]^2))
    pandoc.p(paste("Mean of the slope:", means_slopes[func_ind]))
    pandoc.p(paste("Variance of the slope:", sd_slopes[func_ind]^2))
  }
  func_ind <- func_ind + 1
}

```

##Conclusion: 
The bell shape histogram indicated that the values of $\hat {c_0}$ and $\hat {c_1}$ are normally distributed.

\pagebreak

#Calculation of the standard deviation and mean value of c0 and c1 for every function.
We plotted the values found for analyzing the influence of f(x) on mean and variance.

``` {r comparison-charts, echo=FALSE, dev="png", dpi=300, out.width = '50%'}
function_num <- 1:length(sd_slopes)
plot(sd_slopes ~ function_num, type = 'l', ylim = c(0, max(sd_slopes)), main = "Standard deviation of slopes")
plot(sd_intercepts ~ function_num, type = 'l', ylim =  c(0, max(sd_intercepts)), main = "Standard deviation of y-intercepts")
plot(means_slopes ~ function_num, type = 'l', main = "Means of slopes")
plot(means_intercepts ~ function_num, type = 'l', main = "Means of y-intercepts")
```

##Conclusion:
Mean depends on f(x)
Variance does not.

\pagebreak

We now calculated the covarience of c0 and c1. Since this value is related to the variances, we know that the choice of f(x) will not influence the result.

``` {r covariance-calc, echo=FALSE, results="asis"}
covariance <- cov(dat[,"Slope"], dat[,"Intercept"])
pandoc.p(paste("Covariance between slope and intercept", covariance))
```

##Conclusion:
The value found for the convariance is negligible, and so the values of c0 and c1 are independent.

From the covarience and varience values found we found the formula for the varience of the linear model, depending on the choice of x.

``` {r variance-calc, echo=FALSE, results="asis"}
realdat_coefs <- coef( lm( mytab[,2] ~ mytab[,1]) )

Vc0 <- round(mean(sd_intercepts)**2, 6)
Vc1<- round(mean(sd_slopes)**2, 6)
pandoc.p(paste0("V(f(x)) = ", Vc0, " + ", Vc1, "x^2 + ", "0"))
``` 

From the previous variance formula, we calculated the 90% confidence interval for the linear model.
We plotted the interval together with the best fit linear model and the original data.

``` {r confidence-interval, echo=FALSE, dev="png", dpi=300}
bottom_int <- realdat_coefs[1] + (realdat_coefs[2] * mytab[,1]) + (1.65 * sqrt(Vc0 + Vc1 * (mytab[,1]**2) ))
top_int <- realdat_coefs[1] + (realdat_coefs[2] * mytab[,1]) - (1.65 * sqrt(Vc0 + Vc1 * (mytab[,1]**2) ))

plot( mytab[,1], bottom_int, type = 'l', col = "red", lt=3)
abline(lm( mytab[,2] ~ mytab[,1]))
lines( mytab[,1], top_int, col = "red", lt=3)
points(mytab[,1], mytab[,2], pch=16, cex=0.7)
```

Solving the integral $\hat I = \int (\hat {C_0} + \hat {C_1}x)dx$ for $\hat I$, we found that $\hat I = 2 \hat {C_0}$
Therefore $V(I) = 4V(c0)$
CI = $\hat I \pm z_{0.05}{\hat I}$
``` {r confidence-interval-I, echo=FALSE, results = "asis"}
VI <- 4*Vc0
CI_l <- 2*realdat_coefs[1] - (1.64 * VI)
CI_u <- 2*realdat_coefs[1] + (1.64 * VI)
pandoc.p(paste("CI[", CI_l, ",", CI_u, "]"))
```

# Conclusion
